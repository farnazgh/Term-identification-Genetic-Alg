{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Medical  sciences  have  long  since  established an ethics code for experiments,  to minimize the risk of harm to subjects. Natural  language  processing  (NLP)  used  to involve  mostly  anonymous  corpora,  withthe  goal  of  enriching  linguistic  analysis, and  was  therefore  unlikely  to  raise  ethical  concerns.   As  NLP  becomes  increasingly  wide-spread  and  uses  more  datafrom social media, however, the situationhas changed: the outcome of NLP experiments and applicationscannow have a direct effect on individual users’ lives. Until now, the discourse on this topic in the field has not followed the technological development, while public discourse was often focused on exaggerated dangers.  This position paper tries to take back the initiative and start a discussion. We identify a number of social implications of NLP and discuss their ethical significance,  as well as ways to address them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medical  sciences  have  long  since  established an ethics code for experiments,  to minimize the risk of harm to subjects.',\n",
       " 'Natural  language  processing  (NLP)  used  to involve  mostly  anonymous  corpora,  withthe  goal  of  enriching  linguistic  analysis, and  was  therefore  unlikely  to  raise  ethical  concerns.',\n",
       " 'As  NLP  becomes  increasingly  wide-spread  and  uses  more  datafrom social media, however, the situationhas changed: the outcome of NLP experiments and applicationscannow have a direct effect on individual users’ lives.',\n",
       " 'Until now, the discourse on this topic in the field has not followed the technological development, while public discourse was often focused on exaggerated dangers.',\n",
       " 'This position paper tries to take back the initiative and start a discussion.',\n",
       " 'We identify a number of social implications of NLP and discuss their ethical significance,  as well as ways to address them.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent tokenizer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "\n",
    "sent_list = tokenize.sent_tokenize(text)\n",
    "sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medical  sciences  have  long  since  established an ethics code for experiments  to minimize the risk of harm to subjects',\n",
       " 'Natural  language  processing  NLP  used  to involve  mostly  anonymous  corpora  withthe  goal  of  enriching  linguistic  analysis and  was  therefore  unlikely  to  raise  ethical  concerns',\n",
       " 'As  NLP  becomes  increasingly  widespread  and  uses  more  datafrom social media however the situationhas changed the outcome of NLP experiments and applicationscannow have a direct effect on individual users’ lives',\n",
       " 'Until now the discourse on this topic in the field has not followed the technological development while public discourse was often focused on exaggerated dangers',\n",
       " 'This position paper tries to take back the initiative and start a discussion',\n",
       " 'We identify a number of social implications of NLP and discuss their ethical significance  as well as ways to address them']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "\n",
    "sent_list = [s.translate(str.maketrans('', '', string.punctuation)) for s in sent_list]\n",
    "sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Medical',\n",
       "  'sciences',\n",
       "  'have',\n",
       "  'long',\n",
       "  'since',\n",
       "  'established',\n",
       "  'an',\n",
       "  'ethics',\n",
       "  'code',\n",
       "  'for',\n",
       "  'experiments',\n",
       "  'to',\n",
       "  'minimize',\n",
       "  'the',\n",
       "  'risk',\n",
       "  'of',\n",
       "  'harm',\n",
       "  'to',\n",
       "  'subjects'],\n",
       " ['Natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'NLP',\n",
       "  'used',\n",
       "  'to',\n",
       "  'involve',\n",
       "  'mostly',\n",
       "  'anonymous',\n",
       "  'corpora',\n",
       "  'withthe',\n",
       "  'goal',\n",
       "  'of',\n",
       "  'enriching',\n",
       "  'linguistic',\n",
       "  'analysis',\n",
       "  'and',\n",
       "  'was',\n",
       "  'therefore',\n",
       "  'unlikely',\n",
       "  'to',\n",
       "  'raise',\n",
       "  'ethical',\n",
       "  'concerns'],\n",
       " ['As',\n",
       "  'NLP',\n",
       "  'becomes',\n",
       "  'increasingly',\n",
       "  'widespread',\n",
       "  'and',\n",
       "  'uses',\n",
       "  'more',\n",
       "  'datafrom',\n",
       "  'social',\n",
       "  'media',\n",
       "  'however',\n",
       "  'the',\n",
       "  'situationhas',\n",
       "  'changed',\n",
       "  'the',\n",
       "  'outcome',\n",
       "  'of',\n",
       "  'NLP',\n",
       "  'experiments',\n",
       "  'and',\n",
       "  'applicationscannow',\n",
       "  'have',\n",
       "  'a',\n",
       "  'direct',\n",
       "  'effect',\n",
       "  'on',\n",
       "  'individual',\n",
       "  'users',\n",
       "  '’',\n",
       "  'lives'],\n",
       " ['Until',\n",
       "  'now',\n",
       "  'the',\n",
       "  'discourse',\n",
       "  'on',\n",
       "  'this',\n",
       "  'topic',\n",
       "  'in',\n",
       "  'the',\n",
       "  'field',\n",
       "  'has',\n",
       "  'not',\n",
       "  'followed',\n",
       "  'the',\n",
       "  'technological',\n",
       "  'development',\n",
       "  'while',\n",
       "  'public',\n",
       "  'discourse',\n",
       "  'was',\n",
       "  'often',\n",
       "  'focused',\n",
       "  'on',\n",
       "  'exaggerated',\n",
       "  'dangers'],\n",
       " ['This',\n",
       "  'position',\n",
       "  'paper',\n",
       "  'tries',\n",
       "  'to',\n",
       "  'take',\n",
       "  'back',\n",
       "  'the',\n",
       "  'initiative',\n",
       "  'and',\n",
       "  'start',\n",
       "  'a',\n",
       "  'discussion'],\n",
       " ['We',\n",
       "  'identify',\n",
       "  'a',\n",
       "  'number',\n",
       "  'of',\n",
       "  'social',\n",
       "  'implications',\n",
       "  'of',\n",
       "  'NLP',\n",
       "  'and',\n",
       "  'discuss',\n",
       "  'their',\n",
       "  'ethical',\n",
       "  'significance',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'address',\n",
       "  'them']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sents = [word_tokenize(i) for i in sent_list]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medical',\n",
       " 'sciences',\n",
       " 'have',\n",
       " 'long',\n",
       " 'since',\n",
       " 'established',\n",
       " 'an',\n",
       " 'ethics',\n",
       " 'code',\n",
       " 'for',\n",
       " 'experiments',\n",
       " 'to',\n",
       " 'minimize',\n",
       " 'the',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'harm',\n",
       " 'to',\n",
       " 'subjects']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower casing\n",
    "tokenized_sents = [[t.lower() for t in s] for s in tokenized_sents]\n",
    "tokenized_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining various steps required for the genetic algorithm\n",
    "import random\n",
    "\n",
    "def initilization_of_population(pop_size,n_feat):\n",
    "#     population = []\n",
    "#     for i in range(size):\n",
    "#         chromosome = np.ones(n_feat,dtype=np.bool)\n",
    "#         chromosome[:int(0.3*n_feat)]=False\n",
    "#         np.random.shuffle(chromosome)\n",
    "#         population.append(chromosome)\n",
    "#     return population\n",
    "    return np.random.choice([0, 1], size=(pop_size, n_feat))\n",
    "\n",
    "def fitness_score(population):\n",
    "#     scores = []\n",
    "#     for chromosome in population:\n",
    "        \n",
    "#     scores, population = np.array(scores), np.array(population) \n",
    "#     inds = np.argsort(scores)\n",
    "#     return list(scores[inds][::-1]), list(population[inds,:][::-1])\n",
    "    return [sum(p) for p in population], population\n",
    "\n",
    "def selection(pop_after_fit,n_parents):\n",
    "    population_nextgen = []\n",
    "    for i in range(n_parents):\n",
    "        population_nextgen.append(pop_after_fit[i])\n",
    "    return population_nextgen\n",
    "\n",
    "def crossover(pop_after_sel):\n",
    "    population_nextgen=pop_after_sel\n",
    "    for i in range(len(pop_after_sel)):\n",
    "        child=pop_after_sel[i]\n",
    "        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\n",
    "        population_nextgen.append(child)\n",
    "    return population_nextgen\n",
    "\n",
    "def mutation(pop_after_cross,mutation_rate):\n",
    "    population_nextgen = []\n",
    "    for i in range(0,len(pop_after_cross)):\n",
    "        chromosome = pop_after_cross[i]\n",
    "        for j in range(len(chromosome)):\n",
    "            if random.random() < mutation_rate:\n",
    "                chromosome[j]= not chromosome[j]\n",
    "        population_nextgen.append(chromosome)\n",
    "    #print(population_nextgen)\n",
    "    return population_nextgen\n",
    "\n",
    "def generations(size,n_feat,n_parents,mutation_rate,n_gen):\n",
    "    best_chromo= []\n",
    "    best_score= []\n",
    "    population_nextgen=initilization_of_population(size,n_feat)\n",
    "    for i in range(n_gen):\n",
    "        scores, pop_after_fit = fitness_score(population_nextgen)\n",
    "#         print(scores[:2])\n",
    "        pop_after_sel = selection(pop_after_fit,n_parents)\n",
    "        pop_after_cross = crossover(pop_after_sel)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate)\n",
    "        best_chromo.append(pop_after_fit[0])\n",
    "        best_score.append(scores[0])\n",
    "    return best_chromo,best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1]),\n",
       "  array([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1])],\n",
       " [13,\n",
       "  12,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  14,\n",
       "  11,\n",
       "  10,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  15,\n",
       "  13,\n",
       "  12,\n",
       "  12,\n",
       "  17,\n",
       "  19,\n",
       "  17,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  14,\n",
       "  18,\n",
       "  18,\n",
       "  21,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  19,\n",
       "  16,\n",
       "  19,\n",
       "  16,\n",
       "  17,\n",
       "  19,\n",
       "  17,\n",
       "  15,\n",
       "  17,\n",
       "  16])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent\n",
    "sent = tokenized_sents[0]\n",
    "\n",
    "chromo,score=generations(size=20,n_feat=len(sent),n_parents=5,mutation_rate=0.10,n_gen=20)\n",
    "chromo,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
